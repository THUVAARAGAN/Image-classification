{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Question no 2",
      "provenance": [],
      "authorship_tag": "ABX9TyNSdXj4NhMr7mtagODwEf/x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/THUVAARAGAN/ASSIGNMENT_04/blob/main/Question_no_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjaRmVlJtMj5",
        "outputId": "11634311-cec1-47c8-9a99-3bc54720c8b8"
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "print('x_train: ', x_train.shape)\n",
        "\n",
        "K = len(np.unique(y_train)) # Classes\n",
        "Ntr = x_train.shape[0]\n",
        "Nte = x_test.shape[0]\n",
        "Din = 3072 # CIFAR10\n",
        "# Din = 784 # MINIST\n",
        "\n",
        "\n",
        "# Normalize pixel values\n",
        "#x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "mean_image = np.mean(x_train, axis=0)\n",
        "x_train = x_train - mean_image\n",
        "x_test = x_test - mean_image\n",
        "\n",
        "\n",
        "\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=K)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=K)\n",
        "\n",
        "x_train = np.reshape(x_train,(Ntr,Din))\n",
        "x_test = np.reshape(x_test,(Nte,Din))\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "H=200\n",
        "std=1e-6\n",
        "w1=std*np.random.randn(Din,H)\n",
        "w2=std*np.random.randn(H,K)\n",
        "b1=np.zeros(H)\n",
        "b2=np.zeros(K)\n",
        "batch_size=Ntr\n",
        "\n",
        "\n",
        "iterations =300\n",
        "lr =1.4e-2\n",
        "lr_decay=0.999\n",
        "reg =5e-6\n",
        "loss_history = []\n",
        "train_acc_history = []\n",
        "val_acc_history = []\n",
        "\n",
        "for t in range(iterations):\n",
        "  batch_indices = np.random.choice(Ntr,batch_size)\n",
        "  x=x_train[batch_indices]\n",
        "  y=y_train[batch_indices] \n",
        "\n",
        "  # Forward pass\n",
        "  h=1.0/(1.0+np.exp(-(x.dot(w1)+b1)))\n",
        "  y_pred=h.dot(w2)+b2\n",
        "  loss=1./batch_size*np.square(y_pred-y).sum()+reg*(np.sum(w2*w2)+np.sum(w1*w1))\n",
        "  loss_history.append(loss)\n",
        "\n",
        "  acc=(Ntr-np.count_nonzero(np.abs(np.argmax(y,axis=1)-np.argmax(y_pred,axis=1))))/Ntr\n",
        "  train_acc_history.append(acc)\n",
        "\n",
        "  if t%10==0:\n",
        "    print('iteration %d/%d : loss %f'%(t,iterations,loss))\n",
        "    print('iteration %d/%d : training accuracy %f'%(t,iterations,acc))\n",
        "\n",
        "  # Backward pass\n",
        "  dy_pred=1./batch_size*2.0*(y_pred-y)\n",
        "  dw2=h.T.dot(dy_pred)+reg*w2\n",
        "  db2=dy_pred.sum(axis=0)\n",
        "  dh=dy_pred.dot(w2.T)\n",
        "  dw1=x.T.dot(dh*h*(1-h))+reg*w1\n",
        "  db1=(dh*h*(1-h)).sum(axis=0)\n",
        "\n",
        "\n",
        "  w1-=lr*dw1\n",
        "  w2-=lr*dw2\n",
        "  b1-=lr*db1\n",
        "  b2-=lr*db2\n",
        "  lr*=lr_decay "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 6s 0us/step\n",
            "x_train:  (50000, 32, 32, 3)\n",
            "iteration 0/300 : loss 1.000008\n",
            "iteration 0/300 : training accuracy 0.096120\n",
            "iteration 10/300 : loss 1.102248\n",
            "iteration 10/300 : training accuracy 0.099140\n",
            "iteration 20/300 : loss 0.852445\n",
            "iteration 20/300 : training accuracy 0.257720\n",
            "iteration 30/300 : loss 0.842332\n",
            "iteration 30/300 : training accuracy 0.274120\n",
            "iteration 40/300 : loss 0.832427\n",
            "iteration 40/300 : training accuracy 0.303780\n",
            "iteration 50/300 : loss 0.824636\n",
            "iteration 50/300 : training accuracy 0.318300\n",
            "iteration 60/300 : loss 0.816791\n",
            "iteration 60/300 : training accuracy 0.339060\n",
            "iteration 70/300 : loss 0.809601\n",
            "iteration 70/300 : training accuracy 0.354280\n",
            "iteration 80/300 : loss 0.802936\n",
            "iteration 80/300 : training accuracy 0.360440\n",
            "iteration 90/300 : loss 0.798561\n",
            "iteration 90/300 : training accuracy 0.370180\n",
            "iteration 100/300 : loss 0.797029\n",
            "iteration 100/300 : training accuracy 0.375760\n",
            "iteration 110/300 : loss 0.792191\n",
            "iteration 110/300 : training accuracy 0.376660\n",
            "iteration 120/300 : loss 0.788635\n",
            "iteration 120/300 : training accuracy 0.386100\n",
            "iteration 130/300 : loss 0.785553\n",
            "iteration 130/300 : training accuracy 0.389480\n",
            "iteration 140/300 : loss 0.780549\n",
            "iteration 140/300 : training accuracy 0.399320\n",
            "iteration 150/300 : loss 0.777480\n",
            "iteration 150/300 : training accuracy 0.396420\n",
            "iteration 160/300 : loss 0.774863\n",
            "iteration 160/300 : training accuracy 0.405560\n",
            "iteration 170/300 : loss 0.770214\n",
            "iteration 170/300 : training accuracy 0.416220\n",
            "iteration 180/300 : loss 0.769080\n",
            "iteration 180/300 : training accuracy 0.418500\n",
            "iteration 190/300 : loss 0.774313\n",
            "iteration 190/300 : training accuracy 0.410020\n",
            "iteration 200/300 : loss 0.756842\n",
            "iteration 200/300 : training accuracy 0.435560\n",
            "iteration 210/300 : loss 0.766697\n",
            "iteration 210/300 : training accuracy 0.417980\n",
            "iteration 220/300 : loss 0.760233\n",
            "iteration 220/300 : training accuracy 0.429220\n",
            "iteration 230/300 : loss 0.761387\n",
            "iteration 230/300 : training accuracy 0.431020\n",
            "iteration 240/300 : loss 0.753164\n",
            "iteration 240/300 : training accuracy 0.443900\n",
            "iteration 250/300 : loss 0.750376\n",
            "iteration 250/300 : training accuracy 0.447220\n",
            "iteration 260/300 : loss 0.745923\n",
            "iteration 260/300 : training accuracy 0.451760\n",
            "iteration 270/300 : loss 0.752710\n",
            "iteration 270/300 : training accuracy 0.442040\n",
            "iteration 280/300 : loss 0.751380\n",
            "iteration 280/300 : training accuracy 0.441780\n",
            "iteration 290/300 : loss 0.740360\n",
            "iteration 290/300 : training accuracy 0.463220\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}